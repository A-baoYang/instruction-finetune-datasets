# Datasets for Instruction Fine-Tuning

Since training LLMs on consumer-level GPUs is the trend, but we're still lack of larger high-quality instruction datasets. 
Although some tasks requires private knowledge datasets with professions experiences, which has concern of data privacy, but it's still worth to let them know how to transfer him/her knowledge into instruction datasets and finetune on LLMs.

有鑒於可在消費級顯卡上訓練及運行的大型語言模型的相關研究已經逐漸清晰，但大量且高品質的指令資料集尚未被很好的提供；儘管有些資料集屬於私有領域知識、經驗累積，有使用權上的疑慮；如果讓知識擁有者也能了解怎麼利用自己的知識的打造智能助理，也是未來趨勢之一。這就是本專案的目的。


## Code Generation
- [CodeSearchNet](https://github.com/github/CodeSearchNet#data-details) | [CodeXGLUE](https://github.com/facebookresearch/CodeGen/blob/main/CodeXGLUE/Code-Text/code-to-text/README.md)
- [DS-1000](https://github.com/HKUNLP/DS-1000)
- [ConCode](https://github.com/sriniiyer/concode)(步驟比較多)
- [code-docstring-corpus](https://github.com/EdinburghNLP/code-docstring-corpus)(資料格式不確定)

## General Knowledge

## Public Professional Domain Knowledge

## Private Professional Domain Knowledge

